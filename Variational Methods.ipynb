{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** Let $P, Q$ be probability measures over a set $\\mathcal{X}$. $Q$ is absolutely continuous with respect to $P$, then we define the Kullback-Leibler divergence from $P$ to $Q$ as: \n",
    "\n",
    "$$ \n",
    "D_{KL}\\big (Q \\parallel P \\big) = \\int_{\\mathcal{X}} \\mathrm{log} \\bigg ( \\frac{dQ}{dP} \\bigg) dQ\n",
    "$$\n",
    "\n",
    "where $\\frac{dQ}{dP}$ is the Radon-Nikodym derivative of $Q$ with respect to $P$, provided the right hand side exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem setup** \n",
    "\n",
    "**Remark** When talking for example about distribution $P_{X}(x)$ we mean distribution that has density $P_{X}(x)$ with respect to some measure that we neglect to specify. \n",
    "\n",
    "Generally in variational methods we have some observed value $x^{obs}$ of random variable $X$ and latent (unobserved) random variables $Z$. We know prior distribution $P_{Z}(z)$ and likelihood $P_{X \\mid Z}(x \\mid z)$. We want to estimate posterior distribution $P_{Z \\mid X}(z \\mid x)$. Standard Bayesian rule gives us \n",
    "\n",
    "\\begin{equation}\n",
    "P_{Z \\mid X}(z \\mid x) = \\frac{P_{X \\mid Z}(x \\mid z) P_{Z}(z)}{\\int P_{X \\mid Z}(x \\mid z) P_{Z}(x)dz}\n",
    "\\label{eq:bayes_rule}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "There are multiple ways how to solve this (Gibbs sampling), but sometimes it can be hard. What variational methods propose is to forget about the equation \\eqref{eq:bayes_rule} and instead try to aproximate the posterior $P_{Z\\mid X}(z \\mid x)$ by some distribution $Q_{\\eta}(z)$ that belongs to some sufficiently rich parametric family of distributions (parametrized by $\\eta$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational Bayes Solution**\n",
    "\n",
    "To make the approximation any good, we try to minimize Kullback-Leibler divergence $D_{KL}$ from the true $P_{Z \\mid X}(z \\mid x^{obs})$ to $Q_{\\eta}(z)$. \n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}\\big( Q_{\\eta}(z) \\parallel P_{Z \\mid X}(z \\mid x) \\big) = \\int Q_{\\eta}(z) \\mathrm{log} \\bigg( \\frac{Q_{\\eta}(z)}{P_{Z \\mid X}(z \\mid x^{obs})} \\bigg) dz\n",
    "\\label{eq:variational_KL}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Now we employ a trick and rewrite the right hand side of \\eqref{eq:variational_KL} as \n",
    "\n",
    "\\begin{equation}\n",
    "\\int Q_{\\eta}(z) \\Bigg [ \\mathrm{log} \\bigg( \\frac{Q_{\\eta}(z)}{P_{Z, X}(z, x^{obs})} \\bigg) + \\mathrm{log}\\big( P_{X}(x^{obs})\\big) \\Bigg ] dz\n",
    "\\label{eq:trick_KL}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Let's further denote \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(Q_{\\eta}) = - \\int Q_{\\eta}(z) \\mathrm{log} \\bigg( \\frac{Q_{\\eta}(z)}{P_{Z, X}(z, x^{obs})} \\bigg) dz\n",
    "\\end{equation}\n",
    "\n",
    "So from \\eqref{eq:trick_KL} we have  \n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}\\big( Q_{\\eta}(z) \\parallel P_{Z \\mid X}(z \\mid x) \\big) = \\mathrm{log} \\big ( P_{X}(x^{obs}) \\big ) - \\mathcal{L}(Q_{\\eta})\n",
    "\\label{eq:KL_to_likelihood_and_L}\n",
    "\\tag{4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice, that minimizing $D_{KL}$ is equivalent to maximizing $\\mathcal{L}$ (sometimes called **(variational) lower bound** on the marginal likelihood or **(negative) variational free energy**) since $\\mathrm{log} \\big ( P_{X}(x^{obs}) \\big )$ (sometimes called **log evidence**) does not depend on $Q_{\\eta}$. By appropriate choice of the parametric family $Q_{\\eta}$ the variational lower bound $\\mathcal{L}$ becomes tractable and we are able to maximize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
